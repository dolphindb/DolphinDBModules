/* *
*  @ brief  
*  This module is convenient for operation team to maintain the cluster. 
*  @ Author: DolphinDB
*  @ Last modification time: 2023.12.29
*  @ DolphinDB server version: 2.00.11
*  @ FileName: ops.dos
*/

module ops

/* *
*  @ brief  Cancel background jobs on any node in the cluster.
*  If id is not specified, all running background jobs in the cluster will be canceled.
*  If id is specified, running background jobs whose job IDs contain id will be canceled.
*  @ param
*  id indicates the job ID. It can be obtained by function getRecentJobs().
*  @ Return NULL
*  @ sample usage  cancelJobEx("myjob1")
*/


def cancelJobEx(id=NULL){
    if (id == NULL){// delete all jobs in the cluster
        ids = exec jobId from pnodeRun(getRecentJobs) where endTime = NULL
    }else{// delete job by JobID. 
        ids = exec jobId from pnodeRun(getRecentJobs) where endTime = NULL and jobId = id
    }
    pnodeRun(cancelJob{ids})
}


/* *
*  @ brief  Close inactive sessions.
*  @ param
*  hours determines the interval (in hours) after which a session is determined as inactive. The default value is 12.
*  @ Return  a table of the active connections
*  @ sample usage  closeInactiveSessions(24)
*/


def closeInactiveSessions(hours=12) {
    if(isNull(hours))
       throw "Please input the interval (in hours)"
    sessionIds = exec sessionId from pnodeRun(getSessionMemoryStat) where now() - lastActiveTime > long(hours*60*60*1000)
    pnodeRun(closeSessions{sessionIds})
    return pnodeRun(getSessionMemoryStat)
}


/* *
*  @brief Print the Data Definition Language (DDL) statement of a DFS table.
*  @param
*  database is the absolute path of the folder where the database is stored
*  tableName is the name of dfs table
*  @return print the sql statement of creating a DFS table
*  @sample usage getDDL("dfs://demodb", "pt")
*
*/


def getDDL(database, tableName){
    // Prompt if the table does not exist
    if(!existsTable(database, tableName)){
        return "Please check if the table exists"
    }
    // get the definition  of columns
    tSchema = schema(loadTable(database, tableName))    
    colName = tSchema.colDefs.name
    colType = tSchema.colDefs.typeString
    colName = "`"+concat(colName, "`")
    colType = "["+concat(colType, ",")+"]"

    // get the column name of partition
    partitionCol = tSchema.partitionColumnName
    if(size(partitionCol) > 1)
        partitionCol = "`"+concat(partitionCol, "`")
    else
        partitionCol = "`"+string(partitionCol)
        
     // todo compressMethods	
     
    // sortColumns, keepDuplicates, sortKeyMappingFunction
    if(tSchema.engineType ==`TSDB){
        sortColumns = tSchema.sortColumns
        if(size(sortColumns) > 1)
            sortColumns = "`"+concat(sortColumns, "`")
        else
            sortColumns = "`"+string(sortColumns)
        keepDuplicates = tSchema.keepDuplicates
        // todo sortKeyMappingFunction
    }
          
    // print sql of creating table
    print("db = database("+'\"'+database+'\")')
    print("colName = " + colName)
    print("colType = " + colType)
    print("tbSchema = table(1:0, colName, colType)")
    if(tSchema.engineType !=`TSDB){
        if(tSchema.partitionColumnIndex.size()==1) {
        	if(tSchema.partitionColumnIndex==-1) 
             	print("db.createTable(table=tbSchema,tableName=`" +tableName+")")
            else
                print("db.createPartitionedTable(table=tbSchema,tableName=`" +tableName+",partitionColumns="+partitionCol+")")
        }else{
       	    print("db.createPartitionedTable(table=tbSchema,tableName=`" +tableName+",partitionColumns="+partitionCol+")")
        }
     } else{
       if(tSchema.partitionColumnIndex.size()==1 ){
             if(tSchema.partitionColumnIndex==-1) 
                 print("db.createTable(table=tbSchema,tableName=`"+tableName+",sortColumns=" +sortColumns+",keepDuplicates="+keepDuplicates+")")
             else
                 print("db.createPartitionedTable(table=tbSchema,tableName=`"+tableName+",partitionColumns=" +partitionCol + ",sortColumns="+sortColumns+",keepDuplicates="+keepDuplicates+")")
        	    	
       }else{
             print("db.createPartitionedTable(table=tbSchema,tableName=`"+tableName+",partitionColumns=" +partitionCol + ",sortColumns="+sortColumns+",keepDuplicates="+keepDuplicates+")")
       }
    }
}


/* *
*  @ brief  Get the disk space occupied by the DFS table
*  @ param
*  database is the absolute path of the folder where the database is stored
*  tableName is the name of dfs table
*  byNode is a Boolean value, indicating whether the disk usage is displayed by node
*  @ Return  a table containing the disk space occupied by the DFS table
*  @ sample usage  getTableDiskUsage("dfs://demodb", "machines", true)
*/


def getTableDiskUsage(database, tableName, byNode=false){
    if(byNode == true){
        return select sum(diskUsage\1024\1024\1024) diskGB
                    from pnodeRun(getTabletsMeta{"/"+substr(database, 6)+"/%", tableName, true, -1})
                    group by node
    }else {
        return select sum(diskUsage\1024\1024\1024) diskGB
                    from pnodeRun(getTabletsMeta{"/"+substr(database, 6)+"/%", tableName, true, -1})
    }
}


/* *
*  @ brief  Force delete the recovering partition.
*  @ param  
*  dbPath is the absolute path of the folder where the database is stored
*  tableName is the name of dfs table, required if chunkGranularity is TABLE
*  @ return NULL
*  @ sample usage  dropRecoveringPartitions("dfs://compoDB")
*/


def dropRecoveringPartitions(dbPath , tableName=""){
    db=database(dbPath)
    if(db.schema().chunkGranularity=="TABLE"){
    	if (isNull(tableName) or tableName=="")
    		throw "Please input the table name"
    }
    	
    dbName = substr(dbPath, 5)
    partitions = exec substr(file, strlen(dbName))
                        from rpc(getControllerAlias(), getClusterChunksStatus)
                        where  like(file,  dbName + "%"), state != "COMPLETE"
    if(db.schema().chunkGranularity=="TABLE")                       
        dropPartition(db, partitions, tableName, true)
    else
        dropPartition(db, partitions, , true)
}


/* *
*  @ brief  Get the expiration date of the license on all nodes in the cluster.
*  @ param NULL
*  @ return a table containing expiration date of the license on each node
*  @ sample usage  getAllLicenses(), you should run this function on datanode or controller.
*/

def getAllLicenses(){
    t = table(getNodeAlias() as  node,getLicenseExpiration() as end_date)
    nodes = exec name, host, port from rpc(getControllerAlias(), getClusterPerf{true})
    for(node in nodes){
        if (node[`name] == getNodeAlias()){
            continue
        }
        else{
            try{
                conn = xdb(node[`host], node[`port])
                t1 = remoteRun(
                conn, "table( getNodeAlias() as node ,getLicenseExpiration() as date)")
                t.append!(t1)
            } catch(ex){
                print(ex)
                writeLog(ex)
            }
        }
    }
    return t
}



/* *
*  @ brief  Update the license on all nodes in the cluster.
*  @ param NULL
*  @ return  a table containing expiration date of the license on each node
*  @ sample usage  updateAllLicenses(), Must be done after replacing the license file
*/

def updateAllLicenses(){
    try{
        pnodeRun(getLicenseExpiration)
    }catch(ex){
        print(ex)
        writeLog(ex)
    }
    nodes = exec name, host, port from rpc(getControllerAlias(), getClusterPerf{true})
    localHost = getNodeHost()
    localPort = getNodePort()
    for(node in nodes){
            if(node[`host] == localHost and node[`port] == localPort){
                    updateLicense()
            }
            else{
                     try{
                            conn = xdb(node[`host], node[`port])
                            remoteRun(conn,updateLicense)
                        } catch(ex){
                                    print(ex)
                            writeLog(ex)
                        }
            }
 }
    return getAllLicenses()
}


/* *
*  @ Function name: unsubscribeAll
*  @ Brief: Cancel all subscriptions or the subscriptions of a table on the node
*  @ Param:
*  tbName(default NULL): a string scalar which is the name of a table to be unsubscribed, can be NULL which means to cancel all subscriptions.
*  @ Return: NULL
*  @ Sample usage: unsubscribeAll("table1")
*/

def unsubscribeAll(tbName=NULL) {
	if (tbName!=NULL){
		t = select * from getStreamingStat().pubTables where tableName = tbName
	} else {
		t = getStreamingStat().pubTables
	}
	
	for(row in t){
		tableName = row.tableName
		if(string(row.actions).startsWith("[")) {
			actions = split(substr(row.actions, 1, strlen(row.actions)-2), ",")
		} else {
			actions = [].append!(row.actions)
		}

		for(action in actions){
			unsubscribeTable(tableName=tableName, actionName=action)
		}
	}
}


/**
 * @ brief   Get the performance measures of the cluster within the given monitoring period and interval, save the result to a CSV file.  
 * @ param  
 * monitoringPeriod indicates the monitoring period in seconds, default is 60.
 * scrapeInterval is the scrape interval in seconds, default is 15
 * dir is the directory to  save the CSV fileï¼Œdefault is /tmp.
 * @ return NULL
 */

def gatherClusterPerf(monitoringPeriod=60, scrapeInterval=15, dir="/tmp"){
    targetDir = dir
    if(targetDir == "" || targetDir == string(NULL)){
        targetDir = "/tmp"
    }
    schema0 = schema(rpc(getControllerAlias(), getClusterPerf))
    colNames = [`ts]
    colNames.append!(schema0[`colDefs][`name])
    colTypes = [TIMESTAMP]
    colTypes.append!(schema0[`colDefs][`typeInt])
    statis_table = table(100000: 0, colNames, colTypes)
    startTime = now()
    do {
        insert into statis_table select now() as ts, * from rpc(getControllerAlias(), getClusterPerf)
        sleep(scrapeInterval * 1000)
        elasped_time = (now() - startTime)/1000
    }while(elasped_time < monitoringPeriod)
    saveText(statis_table, targetDir + "/statis.csv")
}


/**
 * @ brief Get the status of workers of the subscriber nodes within the given monitoring period and interval, save the result to a CSV file.
 * @ param: 
 * subNode, the nodeAlias of subscribe node
 * monitoringPeriod indicates the monitoring period in seconds,default is 60.
 * scrapeInterval is the scrape interval in seconds, default is 15
 * dir is the directory to save csv fileï¼Œdefault is /tmp.
 * @ return NULL
 */

def gatherStreamingStat(subNode, monitoringPeriod=60, scrapeInterval=15, dir="/tmp"){
    targetDir = dir
    if(targetDir == "" || targetDir == string(NULL)){
        targetDir = "/tmp"
    }
    schema0 = schema(rpc(subNode, getStreamingStat).subWorkers)
    colNames = [`ts]
    colNames.append!(schema0[`colDefs][`name])
    colTypes = [TIMESTAMP]
    colTypes.append!(schema0[`colDefs][`typeInt])
    stream_statis_table = table(100000: 0, colNames, colTypes)
    startTime = now()
    do {
        insert into stream_statis_table select now() as ts, * from rpc(subNode, getStreamingStat).subWorkers
        sleep(scrapeInterval * 1000)
        elasped_time = (now() - startTime)/1000
    }while(elasped_time < monitoringPeriod)
    saveText(stream_statis_table, targetDir + "/sub_worker_statis.csv")
}

/**
 * @ brief Compares whether the data for two memory tables is the same
 * @ param: 
 * t1, the memory tables
 * t2, the memory tables
 * @ return returns records with different rows of data if different,else print Both tables are identical
 */
def getDifferentData(t1, t2){
        res = each(eqObj, t1.values(), t2.values())
        if(res.all()!=true){
                colIndex = at(res==false)
                comparison = each(eqObj, t1.col(colIndex[0]), t2.col(colIndex[0]))
                rowIndex = at(comparison==false)
                return [t1[rowIndex], t2[rowIndex]]
        }else{
                print "Both tables are identical"
        }
}


def checkOLAPChunkReplicas(dbName, tableName, targetChunkId){
        pathTable = select path, latestPhysicalDir from pnodeRun(getTabletsMeta) where chunkId==uuid(targetChunkId),  tableName==tableName
        if (size(pathTable) == 1) {
            throw "no replica found, no need to check"
        }

        symbolCols=exec  name from loadTable(dbName, tableName).schema().colDefs where typeString ="SYMBOL"
        if(pathTable.size()!=0){
                db=database(dbName)
                colFiles1=exec filename from files(pathTable["path"][0]+"/"+pathTable["latestPhysicalDir"][0]) order by filename
                colFiles2=exec filename from files(pathTable["path"][1]+"/"+pathTable["latestPhysicalDir"][1]) order by filename
                if(colFiles1.size()!=0 && colFiles2.size()!=0){
                        if(eqObj(colFiles1, colFiles2)==false){
                                throw "colFiles on two replicas are not same"
                        }else{
                                res=array(BOOL, 0, size(colFiles1))
                                for(colFile in colFiles1){
                                	  colName=split(colFile,".")[0]
                                	  if (colName in symbolCols){
                                            res1=loadColumn(db, tableName, pathTable["path"][0]+"/"+pathTable["latestPhysicalDir"][0]+"/"+colFile,pathTable["path"][0]+"/chunk.dict")
                                            res2=loadColumn(db, tableName, pathTable["path"][1]+"/"+pathTable["latestPhysicalDir"][1]+"/"+colFile,pathTable["path"][1]+"/chunk.dict")
                                	  }else{
                                             res1=loadColumn(db, tableName, pathTable["path"][0]+"/"+pathTable["latestPhysicalDir"][0]+"/"+colFile)
                                             res2=loadColumn(db, tableName, pathTable["path"][1]+"/"+pathTable["latestPhysicalDir"][1]+"/"+colFile)                               	  	
                                	  }
                                        comparison=eqObj(res1, res2)
                                        res.append!(comparison)
                                }
                        }
                        if(res.all()==true){
                                return true
                        }else{
                                return false
                        }
                }else{
                        throw "colfiles is not exist"
                }
        }else{
                throw "physicalTableDir is not exist"
        }
}


/**
 * @ brief Compares whether the data for two chunk replicas is the same
 * @ param: 
*  dbPath is the absolute path of the folder where the database is stored
*  tableName is the name of dfs table
 * @ return true if the same ,false if different 
 */
def checkChunkReplicas(dbPath, tableName, targetChunkId){
    return checkOLAPChunkReplicas(dbPath, tableName, targetChunkId);
}


/**
*  @ Function name: clearAllSubscriptions
*  @ Brief: clear all subscriptions of the current node
*  @ Param:
*  none
*  @ Return: print cleared subscription information
*  @ Sample usage: clearAllSubscriptions()
*/

def clearAllSubscriptions(){
        if(getStreamingStat().pubTables.rows() > 0){
                do{
                        try{
                                tableName = getStreamingStat().pubTables[0,0]
                                actionName =  getStreamingStat().pubTables[0,3]
                                actionName = strReplace(actionName,"[","")
                                actionName = strReplace(actionName,"]","")
                                arr = actionName.split(',')
                        }
                        catch(ex){
                                print(ex)
                        }
                        for(actionName in arr){
                                try{        
                                        print("unsub: " + tableName + ", "  + actionName)
                                        unsubscribeTable(tableName=tableName, actionName=actionName)
                                        sleep(10)
                                }
                                catch(ex){
                                        print(ex)
                                }
                        }
        
                }
                while(getStreamingStat().pubTables.rows() != 0)
        }
        print("All subscriptions have been cleared !")
}


/**
*  @ Function name: dropAllEngines
*  @ Brief: clear all engines of the current node
*  @ Param:
*  none
*  @ Return: print "All engines have been dropped !"
*  @ Sample usage: dropAllEngines()
*/

def dropAllEngines(){
	if(getStreamEngineStat().rows() > 0){
		engineTypes = getStreamEngineStat().keys()
		for(engineType in engineTypes){
			engineNames = getStreamEngineStat()[engineType].name
			for(name in engineNames){
				try{
					dropStreamEngine(name)
					print("Drop Stream Engine: " + name)
				}
				catch(ex){
					print(ex)
				}
			}
		}
	}
	print("All engines have been dropped !")
}


/**
*  @ Function name: existsShareVariable
*  @ Brief: determine whether each element of a string scalar or vector is a shared variable.
*  @ Param:
*  names: a string scalar/vector indicating object name(s).
*  @ Return: a scalar/vector indicating whether each element of names is a shared variable.
*  @ Sample usage: existsShareVariable("variable1")
*/

def existsShareVariable(names){
     return defined(names, SHARED)
}


/**
*  @ Function name: clearAllSharedTables
*  @ Brief: delete all shared tables of the current node
*  @ Param:
*  none
*  @ Return: print deleted shared table information
*  @ Sample usage: clearAllSharedTables()
*/

def clearAllSharedTables(){
	sharedTables = exec name from objs(true) where form="TABLE", shared=true
	for(sharedTable in sharedTables){
		type = exec type from objs(true) where form="TABLE", shared=true, name=sharedTable
		if(type == "REALTIME"){
			try{
				dropStreamTable(sharedTable)
				print("Drop Shared Table: " + sharedTable)
			}
			catch(ex){
				print(ex)
			}
		}
		else{
			try{
				undef(sharedTable, SHARED)
				print("Drop Shared Table: " + sharedTable)
			}
			catch(ex){
				print(ex)
			}
		}


	}
	print("All shared table have been cleared !")
}


/**
*  @ Function name: clearAllStreamEnv
*  @ Brief: clear all streaming environments of the current node, including subscriptions, engines and shared tables
*  @ Param:
*  none
*  @ Return: print cleared subscription information, engine information and shared table information
*  @ Sample usage: clearAllStreamEnv()
*/

def clearAllStreamEnv(){
        clearAllSubscriptions()
        dropAllEngines()
        clearAllSharedTables()
}


/**
*  @ Function name: getPersistenceTableNames
*  @ Brief: get the table names of all shared stream tables with persistence enabled
*  @ Param:
*  none
*  @ Return: print the table names of all shared stream tables with persistence enabled
*  @ Sample usage: getPersistenceTableNames()
*/

def getPersistenceTableNames(){
	if(getConfigure("persistenceDir") == NULL){
		return NULL
	}else{
		shareNames = exec name from objs(true) where type="REALTIME" and shared=true
		res = array(STRING, 0)
		for(tbName in shareNames){
			try{
				getPersistenceMeta(objByName(tbName))
			}catch(ex){
				continue
			}
			res.append!(tbName)
		}
		return res
	}
}


/**
*  @ Function name: getNonPersistenceTableNames
*  @ Brief: get the table names of all shared stream tables with persistence unenabled
*  @ Param:
*  none
*  @ Return: print the table names of all shared stream tables with persistence unenabled
*  @ Sample usage: getNonPersistenceTableNames()
*/

def getNonPersistenceTableNames(){
	persistenceTableNames = getPersistenceTableNames()
	sharedStreamingTableNames = exec name from objs(true) where type="REALTIME" and shared=true
	return sharedStreamingTableNames[!(sharedStreamingTableNames in persistenceTableNames)]
}


/**
*  @ Function name: getPersistenceStat
*  @ Brief: get the status of all shared stream tables with persistence enabled
*  @ Param:
*  none
*  @ Return: return metadata of all shared stream tables with persistence enabled
*  @ Sample usage: getPersistenceStat()
*/

def getPersistenceStat(){
	tableNames = getPersistenceTableNames()
	resultColNames = ["tablename","lastLogSeqNum","sizeInMemory","asynWrite","totalSize","raftGroup","compress","memoryOffset","sizeOnDisk","retentionMinutes","persistenceDir","hashValue","diskOffset"]
	resultColTypes = ["STRING", "LONG","LONG","BOOL","LONG","INT","BOOL","LONG","LONG","LONG","STRING","INT","LONG"]
	result = table(1:0, resultColNames, resultColTypes)
	for(tbname in tableNames){
		tbStat = getPersistenceMeta(objByName(tbname))
		tbStat["tablename"] = tbname
		result.tableInsert(tbStat)	
	}
	return result
}


/**
*  @ Function name: getNonPersistenceTableStat
*  @ Brief: get the status of all shared stream tables with persistence unenabled
*  @ Param:
*  none
*  @ Return: return metadata of all shared stream tables with persistence unenabled
*  @ Sample usage: getNonPersistenceTableStat()
*/

def getNonPersistenceTableStat(){
	tableNames = getNonPersistenceTableNames()
	return select name as TableName,  rows, columns, bytes from objs(true) where name in tableNames
}


/**
*  @ Function name: clearAllPersistenceTables
*  @ Brief: delete all stream tables with persistence enabled
*  @ Param:
*  none
*  @ Return: none
*  @ Sample usage: clearAllPersistenceTables()
*/

def clearAllPersistenceTables(){
	tableNames = getPersistenceTableNames()
	for(tbname in tableNames){
		dropStreamTable(tbname)
	}
}


/* *
*  @brief Return the Data Definition Language (DDL) statement for a specific database as a string.
*  
*  @param dbName - The absolute path of the database.
*  @return - scalar[STRING], a string containing the DDL statement.
*  
*  @sample usage 
*  getDatabaseDDL("dfs://demodb")
*/

def getDatabaseDDL(dbName){
	if(!existsDatabase(dbName)){
		print("databaseName: " + dbName +", does not exist.")
        		return "Warning: database does not exist."
		}
	
	res = array(STRING)
	dbSchema = schema(database(dbName))
	partitionSchema = dbSchema.partitionSchema
	partitionSites  = snippet(dbSchema.partitionSites)
	dbpartitionTypeName  = dbSchema.partitionTypeName
	partitionColumnType = dbSchema.partitionColumnType
	dbatomic = dbSchema.atomic
	engineType = dbSchema.engineType
	dbchunkGranularity = dbSchema.chunkGranularity
	clusterReplicationEnabled = dbSchema.clusterReplicationEnabled
	dbpartitionType = dbSchema.partitionType
	paraCount = size(defs(`database).syntax[0].split(","))

	directory = "directory = '" + dbName +"'"
	if(engineType==NULL){
		engineType = "OLAP"
		}
	engine = "engine= `" + engineType
	atomic = "atomic = `" + dbatomic
	
	if (size(dbpartitionTypeName)==1){
		partitionType = "partitionType = " +dbpartitionTypeName		
		if (dbpartitionTypeName=="HASH"){
			partitionScheme = "partitionScheme = ["+partitionColumnType+","+partitionSchema+"]"
			}
		else if (dbpartitionTypeName=="SEQ"){
			partitionScheme = "partitionScheme = "+partitionSchema
			}
		else if (dbpartitionTypeName=="RANGE"){						
			f = def(x){a = ();return  snippet(a.append!(x)).split("(")[1].split(")")[0]}
			partitionSchemaTmp = each(f,partitionSchema)		
			partitionScheme =  "partitionScheme =[" + string(concat(partitionSchemaTmp,","))+"]"+"$"+string(partitionColumnType)
			}
		else{			
			f = def(x,partitionColumnType){
				a = ();
				if(partitionColumnType==17){
					return  snippet(a.append!(x)).split("(")[1].split(")")[0]+"$"+string(18)
					}
				else {
					return  snippet(a.append!(x)).split("(")[1].split(")")[0]+"$"+string(partitionColumnType)
					}
				}
			partitionSchemaTmp = each(f{,partitionColumnType},partitionSchema)		
			partitionScheme =  "partitionScheme =[" + string(concat(partitionSchemaTmp,","))+"]"
			}
		if(partitionSites == NULL){
			if(paraCount==6){
				tmp = "database(" + directory + ", " + partitionType + ", " + partitionScheme + ", " + engine + ", " + atomic+ ")"
				}
			else{
				chunkGranularity = "chunkGranularity = `"+ dbchunkGranularity 
				tmp = "database(" + directory + ", " + partitionType + ", " + partitionScheme + ", " + engine + ", " + atomic + ", " + chunkGranularity +")"
				}
			}
		else{
			locations = "locations = " +snippet(partitionSites)
			if(paraCount==6){
				tmp = "database(" + directory + ", " + partitionType + ", " + partitionScheme + ", " + locations + ", "+ engine + ", " + atomic+ ")"
				}
			else{
				chunkGranularity = "chunkGranularity = `"+ dbchunkGranularity
				tmp = "database(" + directory + ", " + partitionType + ", " + partitionScheme + ", " + locations + ", "+ engine + ", " + atomic+  ", " + chunkGranularity +")"
				}
			}
		}
	else{
		partitionType = "partitionType = COMPO" 
		for (i in 0..(size(dbpartitionTypeName)-1)){
			if (dbpartitionTypeName[i]=="HASH"){
				partitonTmp = "db"+ i +" =database(, partitionType = " +  dbpartitionTypeName[i] +", partitionScheme = ["+partitionColumnType[i]+","+partitionSchema[i]+"])"
			}
			else if (dbpartitionTypeName[i]=="SEQ"){
				partitonTmp = "db"+ i +" =database(, partitionType = " +  dbpartitionTypeName[i] +", partitionScheme =" +partitionSchema[i] +")"
			}
			else if (dbpartitionTypeName[i]=="RANGE"){						
			f = def(x){a = ();return  snippet(a.append!(x)).split("(")[1].split(")")[0]}
			partitionSchemaTmp = each(f,partitionSchema[i])		
			partitonTmp = "db"+ i +" =database(, partitionType = " +  dbpartitionTypeName[i] +", partitionScheme =[" + string(concat(partitionSchemaTmp,","))+"]"+"$"+string(partitionColumnType[i])+")"
			}
			else {		
				f = def(x,partitionColumnType){
					a = ();
					if(partitionColumnType==17){
						return  snippet(a.append!(x)).split("(")[1].split(")")[0]+"$"+string(18)
						}
					else {
						return  snippet(a.append!(x)).split("(")[1].split(")")[0]+"$"+string(partitionColumnType)
						}
					}
				partitionSchemaTmp = each(f{,partitionColumnType[i]},partitionSchema[i])		
				partitonTmp = "db"+ i +" =database(, partitionType = " +  dbpartitionTypeName[i] +", partitionScheme =[" + string(concat(partitionSchemaTmp,","))+"])"
			}
		res.append!(partitonTmp)
		}
		partitionScheme = "partitionScheme = ["+concat("db"+string(0..(size(dbpartitionTypeName)-1)),",")+"]"
		if(partitionSites == NULL){
			if(paraCount==6){
				tmp = "database(" + directory + ", " + partitionType + ", " + partitionScheme + ", " + engine + ", " + atomic+ ")"
			}
			else{
				chunkGranularity = "chunkGranularity = `"+ dbchunkGranularity 
				tmp = "database(" + directory + ", " + partitionType + ", " + partitionScheme + ", " + engine + ", " + atomic + ", " + chunkGranularity +")"
			}
		}
		else{
			locations = "locations = " +snippet(partitionSites)
			if(paraCount==6){
				tmp = "database(" + directory + ", " + partitionType + ", " + partitionScheme + ", " + locations + ", "+ engine + ", " + atomic+ ")"
			}  
			else{
				chunkGranularity = "chunkGranularity = `"+ dbchunkGranularity
				tmp = "database(" + directory + ", " + partitionType + ", " + partitionScheme + ", " + locations + ", "+ engine + ", " + atomic+  ", " + chunkGranularity +")"
			}
		}		
	}
	res.append!(tmp)
	return res.concat(";")
}

/* *
*  @brief Return the Data Definition Language (DDL) statement of a specific database table as a string.
*  
*  @param dbName - The absolute path of the database.
*  @param tbName - The name of the database table.
*  @return - scalar[STRING], a string containing the DDL statement.
*  
*  @sample usage 
*  getDBTableDDL("dfs://demodb","tb")
*/
def getDBTableDDL(dbName,tbName){
	if(!existsTable(dbName, tbName)){
    		print("tableName: " + tbName +", does not exist in database : " +dbName)
        		return "Warning: table does not exist."
    	}
	
	tbSchema = schema(loadTable(dbName,tbName))
	colDefs = tbSchema.colDefs

	dbHandle = "dbHandle = database('" + dbName +"')"
	tableSchema = "table = table(1:0, " + snippet(colDefs.name) +"," + snippet(colDefs.typeString) +")"
	tableName = "tableName = '" + snippet(tbName) +"'"
	compressMethods = "compressMethods = dict(" + snippet(tbSchema.compressMethods.name) +"," +snippet(tbSchema.compressMethods.compressMethods)+")"

	engineT = tbSchema.engineType
	paraCountDimension = size(defs(`createTable).syntax[0].split(","))
	paraCountPartition = size(defs(`createPartitionedTable).syntax[0].split(","))
	boolTrans = dict(0 1, `false`true)

	if (tbSchema.partitionColumnIndex[0]==-1){
		if ((engineT =='OLAP') or (engineT == NULL)){
			res = "createTable("+dbHandle+","+tableSchema+","+tableName+","+compressMethods+")"
			}
		else{
			if(paraCountDimension==6){
				sortColumns = "sortColumns = " + snippet(tbSchema.sortColumns) 
				keepDuplicates = "keepDuplicates = " +tbSchema.keepDuplicates
				res = "createTable("+dbHandle+","+tableSchema+","+tableName+","+compressMethods+","+sortColumns+","+keepDuplicates+")"
			}
			if(paraCountDimension==7)
				sortColumns = "sortColumns = " + snippet(tbSchema.sortColumns)
				keepDuplicates = "keepDuplicates = " +tbSchema.keepDuplicates
				softDelete = "softDelete = " + boolTrans[tbSchema.softDelete]
				res = "createTable("+dbHandle+","+tableSchema+","+tableName+","+compressMethods+","+sortColumns+","+keepDuplicates+","+softDelete+")"
			}
		}
	else{
		if ((engineT =='OLAP') or (engineT == NULL)){
			partitionColumns = "partitionColumns =" + snippet(array(STRING).append!(tbSchema.partitionColumnName))
			res = "createPartitionedTable("+dbHandle+","+tableSchema+","+tableName+","+partitionColumns+","+compressMethods+")"
			}
		else{
			if(paraCountPartition==8){
				sortColumns = "sortColumns = " + snippet(tbSchema.sortColumns) 
				keepDuplicates = "keepDuplicates = " +tbSchema.keepDuplicates
				partitionColumns = "partitionColumns =" + snippet(array(STRING).append!(tbSchema.partitionColumnName))
				if (any(tbSchema.keys()==`sortKeyMappingFunction)){
					sortKeyMappingFunction ="sortKeyMappingFunction = ["+ concat(tbSchema.sortKeyMappingFunction,",") + "]"
					res = "createPartitionedTable("+dbHandle+","+tableSchema+","+tableName+","+partitionColumns+","+compressMethods+","+sortColumns+","+keepDuplicates+","+sortKeyMappingFunction+")"
				}
				else{
					res = "createPartitionedTable("+dbHandle+","+tableSchema+","+tableName+","+partitionColumns+","+compressMethods+","+sortColumns+","+keepDuplicates+")"
				}
			}
			if(paraCountPartition==9){
				sortColumns = "sortColumns = " + snippet(tbSchema.sortColumns)
				keepDuplicates = "keepDuplicates = " +tbSchema.keepDuplicates
				softDelete = "softDelete = " + boolTrans[tbSchema.softDelete]
				partitionColumns = "partitionColumns =" + snippet(array(STRING).append!(tbSchema.partitionColumnName))
				if (any(tbSchema.keys()==`sortKeyMappingFunction)){
					sortKeyMappingFunction ="sortKeyMappingFunction = ["+ concat(tbSchema.sortKeyMappingFunction,",") + "]"
					res = "createPartitionedTable("+dbHandle+","+tableSchema+","+tableName+","+partitionColumns+","+compressMethods+","+sortColumns+","+keepDuplicates+","+sortKeyMappingFunction+","+softDelete+")"
				}
				else{
					res = "createPartitionedTable("+dbHandle+","+tableSchema+","+tableName+","+partitionColumns+","+compressMethods+","+sortColumns+","+keepDuplicates+","+softDelete+")"
				}
			}		
		}		
	}
	if(any(split((typestr(res))," ")==`VECTOR)){
		return res[0]
		}
	else{
		return res
	}
}



/* *
*  @ Function name: getChunkUsageByNode
*  @ Brief: Check if the data in a table is evenly distributed across all nodes
*  @ Param:
*  dbName: the name of one database
*  tbName: the name of one table in the database
*  @ Return: a table that contains the number of chunks, disk usage, and the proportion of disk usage on each node
*  @ Sample usage: 
*  getChunkUsageByNode("dfs://TL_level2", "trade")
*/
def getChunkUsageByNode(dbName, tbName) {
	dfsPath = dbName.strReplace("dfs://","/")+"%"
	datanode = exec node from pnodeRun(getNodeType) where value=0 or value =3
	
	checkRes = select count(*) as count, sum(diskUsage) as diskUsage from pnodeRun(getTabletsMeta{dfsPath, tbName, true, -1},datanode) group by node
	checkRes = select *, diskUsage\sum(diskUsage) as percent from checkRes order by node
	return checkRes
}


/* *
*  @ Function name: getChunkUsageByVolume
*  @ Brief: Check if the data in a table is evenly distributed across all volumes
*  @ Param:
*  dbName: the name of one database
*  tbName: the name of one table in the database
*  node(default NULL): the alias scalar or vector of datanodes
*  @ Return: a table that contains the number of chunks, disk usage, and the proportion of disk usage on each node by node and volume
*  @ Sample usage: 
*  getChunkUsageByVolume("dfs://TL_level2", "trade", `dnode2`dnode3)
*/
def getChunkUsageByVolume(dbName, tbName, node = NULL) {
	dfsPath = dbName.strReplace("dfs://", "/")+"%"
	datanode = node
	if (node.size() == 1){
		if (node==NULL){
			datanode = exec node from pnodeRun(getNodeType) where value=0 or value =3
		}
	}
	checkRes = select path.substr(0, path.strpos("/CHUNKS")) as path, node, diskUsage from pnodeRun(getTabletsMeta{dfsPath, tbName, true, -1}, datanode)
	checkRes = select count(*), sum(diskUsage) as diskUsage from checkRes group by node, path
	checkRes = select *, diskUsage\sum(diskUsage) as percent from checkRes context by node
	return checkRes
}